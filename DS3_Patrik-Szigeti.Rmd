---
title: "Data Science 3: Homework Assignment 1"
author: "Patrik Szigeti"
date: '03/04/2020'
output: html_document
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r, include = FALSE, message = FALSE, warning = FALSE}
# Load libraries
library(data.table)
library(magrittr)
library(caret)
library(rpart)
library(rpart.plot)
library(xgboost)
library(ranger)
library(gbm)
library(ISLR)
library(skimr)
library(ROCR)
```

# 1. Tree ensemble models

```{r}
# Read in the OJ dataset
oj_data <- data.table(OJ)
```

```{r, include=FALSE}
skim(oj_data)
```

Our goal is to predict `Purchase`, which is a factor with two possible outcomes: __CH__ (Citrus Hill) or __MM__ (Minute Maid) orange juices. We want to know which orange juice is picked in a certain situation. We have 1070 observations and 18 variables in the dataset.

#### a. Create a training data of 75% and keep 25% of the data as a test set. Train a decision tree as a benchmark model. Plot the final model and interpret the result.

* Create training data (75%) and test data (25%)

```{r}
set.seed(1234)
train_indices <- createDataPartition(
  y = oj_data[["Purchase"]],
  times = 1,
  p = 0.75,
  list = FALSE
)

oj_data_train <- oj_data[train_indices, ]
oj_data_test <- oj_data[-train_indices, ]
```

* Train a decision tree as the benchmark model

```{r}
train_control <- trainControl(
  method = "repeatedcv",
  number = 5,
  repeats = 3
)

set.seed(1234)
simple_tree_model <- train(
  Purchase ~ .,
  method = "rpart",
  data = oj_data_train,
  tuneGrid = data.frame(cp = c(0.01, 0.02, 0.05)),
  trControl = train_control)
simple_tree_model
```

* Plot the final model

```{r fig.align="center"}
rpart.plot(simple_tree_model[["finalModel"]])
```

The simple decision tree considers three variables as splitting points: `LoyalCH` (customer brand loyalty for CH), `PriceDiff` (sale price of MM - sale price of CH) and `SpecialCH` (indicator of special on CH).

* Not surprisingly, if someone is more loyal to CH, they tend to choose CH (in 64% of the cases).
* Out of the 32% whose loyalty to CH is less than 0.76 but greater than or equal to 0.45, 4% of the population will choose MM when CH costs more by at least 0.16, but 27% will stick by CH.
* Out of the 36% who is less loyal to CH, only 3% will buy CH if there's a special sale on CH, 33% of the whole population isn't influenced by the sale, and will still buy MM.

#### b. Investigate tree ensemble models: random forest, gradient boosting machine, XGBoost. Try various tuning parameter combinations and select the best model using cross-validation.

* Random forest

Since "variance" splitrule is only applicable to regression data, and this is a classification problem, I tried the three possible splitting rules from the `ranger` package: `gini`, `extratrees` and `hellinger` to determine which one performs best in this case.

```{r}
tune_grid_gini <- expand.grid(
  .mtry = c(2, 3, 5, 7, 9, 11, 13, 15),
  .splitrule = "gini",
  .min.node.size = c(5, 10)
)

tune_grid_et <- expand.grid(
  .mtry = c(2, 3, 5, 7, 9, 11, 13, 15),
  .splitrule = "extratrees",
  .min.node.size = c(5, 10)
)

tune_grid_h <- expand.grid(
  .mtry = c(2, 3, 5, 7, 9, 11, 13, 15),
  .splitrule = "hellinger",
  .min.node.size = c(5,10)
)

# Random forest with "gini" splitrule
set.seed(1234)
rf_model_gini <- train(
  Purchase ~ .,
  method = "ranger",
  data = oj_data_train,
  trControl = train_control,
  tuneGrid = tune_grid_gini,
  importance = "impurity"
)

# Random forest with "extratrees" splitrule
set.seed(1234)
rf_model_et <- train(
  Purchase ~ .,
  method = "ranger",
  data = oj_data_train,
  trControl = train_control,
  tuneGrid = tune_grid_et,
  importance = "impurity")

# Random forest with "hellinger" splitrule
set.seed(1234)
rf_model_h <- train(
  Purchase ~ .,
  method = "ranger",
  data = oj_data_train,
  trControl = train_control,
  tuneGrid = tune_grid_h,
  importance = "impurity")
```

Now let's see how they compare to each other.

```{r}
resamples(list("gini" = rf_model_gini, 
               "extratrees" = rf_model_et, 
               "hellinger" = rf_model_h)) %>% 
  summary()
```

There are only small differences, but based on the mean accuracy, `extratrees` does slightly better than the default Gini-index, so I'll use that. `extratrees` basically splits nodes by choosing cut-points fully at random as described [here](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.65.7485&rep=rep1&type=pdf).

```{r}
rf_model <- rf_model_et
```

* Gradient boosting machine

```{r}
gbm_grid <- expand.grid(
  n.trees = c(750, 1000, 1250), 
  interaction.depth = c(1, 2, 3), 
  shrinkage = c(0.005, 0.01, 0.015, 0.02),
  n.minobsinnode = c(1))

set.seed(1234)
gbm_model <- train(
  Purchase ~ .,
  method = "gbm",
  data = oj_data_train,
  trControl = train_control,
  tuneGrid = gbm_grid,
  verbose = FALSE)

gbm_model
```

* XGBoost

```{r}
xgb_grid <- expand.grid(
  nrounds = c(500, 1000),
  max_depth = c(2, 3, 5),
  eta = c(0.01, 0.05),
  gamma = 0,
  colsample_bytree = c(0.5, 0.7),
  min_child_weight = 1,
  subsample = c(0.5))

set.seed(1234)
xgboost_model <- train(
  Purchase ~ .,
  method = "xgbTree",
  data = oj_data_train,
  trControl = train_control,
  tuneGrid = xgb_grid)

xgboost_model
```

#### c. Compare different models with the resamples function (make sure to set the same seed before model training for all 3 models). Is any of these giving significantly different predictive power than the others?

```{r}
resamples_object <- resamples(list("simple_tree" = simple_tree_model,
                                   "random_forest" = rf_model,
                                   "gbm" = gbm_model,
                                   "xgboost" = xgboost_model))
summary(resamples_object)
```

Based on the mean accuracy, random forest has the worst predictive power out of the four models, but even that isn't significantly worse. It's interesting to see, however, that even the simple tree model performs better on the train dataset. GBM and XGBoost perform the best (both with 0.83 accuracy), but the trade-off for XGBoost's computational needs doesn't seem to be worth it, as GBM is slightly better. I'm selecting _GBM_ as the best predictor.

#### d. Choose the best model and plot ROC curve for the best model on the test set. Calculate and interpret AUC.

```{r fig.align="center"}
predicted_probabilities <- predict(gbm_model, newdata = oj_data_test, type = "prob")
rocr_prediction <- prediction(predicted_probabilities[["MM"]], oj_data_test[["Purchase"]])
plot(performance(rocr_prediction, "tpr", "fpr"), col = "black")
abline(a = 0, b = 1, col = "#8AB63F")
```

```{r}
performance(rocr_prediction, measure = "auc")@y.values[[1]]
```

The green line represents a uniformative test where AUC = 0.5. The AUC of the GBM model is 0.903, which indicates that we can predict the type of orange juice a customer is going to buy pretty well, the model is capable of distinguishing between the two classes with more than a 90% chance.

#### e. Inspect variable importance plots for the 3 models. Are similar variables found to be the most important for the 3 models?

* Random forest

```{r fig.align="center"}
plot(varImp(rf_model))
```

The most important variable for the random forest is by far `LoyalCH`, which measures customer brand loyalty for CH. It's followed by `WeekofPurchase` and `StoreID`. Anything price-related is less important than these variables.

* GBM

```{r fig.align="center"}
plot(varImp(gbm_model))
```

For the GBM model, `LoyalCH` is still the most important factor, however, `PriceDiff` preceeds `WeekofPurchase` and `Store ID`. With GBM, the order of `ListPriceDiff` and `SalePriceMM` are switched compared to the random forest.

* XGBoost

```{r fig.align="center"}
plot(varImp(xgboost_model))
```

The 6 most important variables are identical between GBM and XGBoost.

Altogether, each of the three models operates with very similar variable importances with minor differences, with `LoyalCH` being the most important variable.

