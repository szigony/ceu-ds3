---
title: "Data Science 3: Homework Assignment 1"
author: "Patrik Szigeti"
date: '03/04/2020'
output: html_document
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r, include = FALSE, message = FALSE, warning = FALSE}
# Load libraries
library(data.table)
library(magrittr)
library(caret)
library(rpart)
library(rpart.plot)
library(xgboost)
library(ranger)
library(gbm)
library(ISLR)
library(skimr)
library(ROCR)
```

# 1. Tree ensemble models

```{r}
# Read in the OJ dataset
oj_data <- data.table(OJ)
```

```{r, include=FALSE}
skim(oj_data)
```

Our goal is to predict `Purchase`, which is a factor with two possible outcomes: __CH__ (Citrus Hill) or __MM__ (Minute Maid) orange juices. We want to know which orange juice is picked in a certain situation. We have 1070 observations and 18 variables in the dataset.

#### a. Create a training data of 75% and keep 25% of the data as a test set. Train a decision tree as a benchmark model. Plot the final model and interpret the result.

* Create training data (75%) and test data (25%)

```{r}
set.seed(1234)
train_indices <- createDataPartition(
  y = oj_data[["Purchase"]],
  times = 1,
  p = 0.75,
  list = FALSE
)

oj_data_train <- oj_data[train_indices, ]
oj_data_test <- oj_data[-train_indices, ]
```

* Train a decision tree as the benchmark model

```{r}
train_control <- trainControl(
  method = "repeatedcv",
  number = 5,
  repeats = 3
)

set.seed(1234)
simple_tree_model <- train(
  Purchase ~ .,
  method = "rpart",
  data = oj_data_train,
  tuneGrid = data.frame(cp = c(0.01, 0.02, 0.05)),
  trControl = train_control)
simple_tree_model
```

* Plot the final model

```{r fig.align="center"}
rpart.plot(simple_tree_model[["finalModel"]])
```

The simple decision tree considers three variables as splitting points: `LoyalCH` (customer brand loyalty for CH), `PriceDiff` (sale price of MM - sale price of CH) and `SpecialCH` (indicator of special on CH).

* Not surprisingly, if someone is more loyal to CH, they tend to choose CH (in 64% of the cases).
* Out of the 32% whose loyalty to CH is less than 0.76 but greater than or equal to 0.45, 4% of the population will choose MM when CH costs more by at least 0.16, but 27% will stick by CH.
* Out of the 36% who is less loyal to CH, only 3% will buy CH if there's a special sale on CH, 33% of the whole population isn't influenced by the sale, and will still buy MM.

#### b. Investigate tree ensemble models: random forest, gradient boosting machine, XGBoost. Try various tuning parameter combinations and select the best model using cross-validation.

* Random forest

Since "variance" splitrule is only applicable to regression data, and this is a classification problem, I tried the three possible splitting rules from the `ranger` package: `gini`, `extratrees` and `hellinger` to determine which one performs best in this case.

```{r}
tune_grid_gini <- expand.grid(
  .mtry = c(2, 3, 5, 7, 9, 11, 13, 15),
  .splitrule = "gini",
  .min.node.size = c(5, 10)
)

tune_grid_et <- expand.grid(
  .mtry = c(2, 3, 5, 7, 9, 11, 13, 15),
  .splitrule = "extratrees",
  .min.node.size = c(5, 10)
)

tune_grid_h <- expand.grid(
  .mtry = c(2, 3, 5, 7, 9, 11, 13, 15),
  .splitrule = "hellinger",
  .min.node.size = c(5,10)
)

# Random forest with "gini" splitrule
set.seed(1234)
rf_model_gini <- train(
  Purchase ~ .,
  method = "ranger",
  data = oj_data_train,
  trControl = train_control,
  tuneGrid = tune_grid_gini,
  importance = "impurity"
)

# Random forest with "extratrees" splitrule
set.seed(1234)
rf_model_et <- train(
  Purchase ~ .,
  method = "ranger",
  data = oj_data_train,
  trControl = train_control,
  tuneGrid = tune_grid_et,
  importance = "impurity")

# Random forest with "hellinger" splitrule
set.seed(1234)
rf_model_h <- train(
  Purchase ~ .,
  method = "ranger",
  data = oj_data_train,
  trControl = train_control,
  tuneGrid = tune_grid_h,
  importance = "impurity")
```

Now let's see how they compare to each other.

```{r}
resamples(list("gini" = rf_model_gini, 
               "extratrees" = rf_model_et, 
               "hellinger" = rf_model_h)) %>% 
  summary()
```

There are only small differences, but based on the mean accuracy, `extratrees` does slightly better than the default Gini-index, so I'll use that. `extratrees` basically splits nodes by choosing cut-points fully at random as described [here](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.65.7485&rep=rep1&type=pdf).

```{r}
rf_model <- rf_model_et
```

* Gradient boosting machine

```{r}
gbm_grid <- expand.grid(
  n.trees = c(750, 1000, 1250), 
  interaction.depth = c(1, 2, 3), 
  shrinkage = c(0.005, 0.01, 0.015, 0.02),
  n.minobsinnode = c(1))

set.seed(1234)
gbm_model <- train(
  Purchase ~ .,
  method = "gbm",
  data = oj_data_train,
  trControl = train_control,
  tuneGrid = gbm_grid,
  verbose = FALSE)

gbm_model
```

* XGBoost

```{r}
xgb_grid <- expand.grid(
  nrounds = c(500, 1000),
  max_depth = c(2, 3, 5),
  eta = c(0.01, 0.05),
  gamma = 0,
  colsample_bytree = c(0.5, 0.7),
  min_child_weight = 1,
  subsample = c(0.5))

set.seed(1234)
xgboost_model <- train(
  Purchase ~ .,
  method = "xgbTree",
  data = oj_data_train,
  trControl = train_control,
  tuneGrid = xgb_grid)

xgboost_model
```

#### c. Compare different models with the resamples function (make sure to set the same seed before model training for all 3 models). Is any of these giving significantly different predictive power than the others?

```{r}
resamples_object <- resamples(list("simple_tree" = simple_tree_model,
                                   "random_forest" = rf_model,
                                   "gbm" = gbm_model,
                                   "xgboost" = xgboost_model))
summary(resamples_object)
```

Based on the mean accuracy, random forest has the worst predictive power out of the four models, but even that isn't significantly worse. It's interesting to see, however, that even the simple tree model performs better on the train dataset. GBM and XGBoost perform the best (both with 0.83 accuracy), but the trade-off for XGBoost's computational needs doesn't seem to be worth it, as GBM is slightly better. I'm selecting _GBM_ as the best predictor.

#### d. Choose the best model and plot ROC curve for the best model on the test set. Calculate and interpret AUC.

```{r fig.align="center"}
predicted_probabilities <- predict(gbm_model, newdata = oj_data_test, type = "prob")
rocr_prediction <- prediction(predicted_probabilities[["MM"]], oj_data_test[["Purchase"]])
plot(performance(rocr_prediction, "tpr", "fpr"), col = "black")
abline(a = 0, b = 1, col = "#8AB63F")
```

```{r}
performance(rocr_prediction, measure = "auc")@y.values[[1]]
```

The green line represents a uniformative test where AUC = 0.5. The AUC of the GBM model is 0.903, which indicates that we can predict the type of orange juice a customer is going to buy pretty well, the model is capable of distinguishing between the two classes with more than a 90% chance.

#### e. Inspect variable importance plots for the 3 models. Are similar variables found to be the most important for the 3 models?

* Random forest

```{r fig.align="center"}
plot(varImp(rf_model))
```

The most important variable for the random forest is by far `LoyalCH`, which measures customer brand loyalty for CH. It's followed by `WeekofPurchase` and `StoreID`. Anything price-related is less important than these variables.

* GBM

```{r fig.align="center"}
plot(varImp(gbm_model))
```

For the GBM model, `LoyalCH` is still the most important factor, however, `PriceDiff` preceeds `WeekofPurchase` and `Store ID`. With GBM, the order of `ListPriceDiff` and `SalePriceMM` are switched compared to the random forest.

* XGBoost

```{r fig.align="center"}
plot(varImp(xgboost_model))
```

The 6 most important variables are identical between GBM and XGBoost.

Altogether, each of the three models operates with very similar variable importances with minor differences, with `LoyalCH` being the most important variable.

# 2. Variable importance profiles

```{r}
hitters_data <- data.table(Hitters)
hitters_data <- hitters_data[!is.na(Salary)]
hitters_data[, log_salary := log(Salary)]
hitters_data[, Salary := NULL]
```

#### a. Train two random forest models: one with mtry = 2 and another with mtry = 10 (use the whole dataset and don't use cross-validation). Inspect variable importance profiles. What do you see in terms of how important the first few variables are relative to each other?

```{r}
# mtry = 2
tune_grid_mtry2 <- expand.grid(
  .mtry = 2,
  .splitrule = "variance",
  .min.node.size = 1
)

set.seed(1234)
rf_model_mtry2 <- train(
  log_salary ~ .,
  method = "ranger",
  data = hitters_data,
  tuneGrid = tune_grid_mtry2,
  importance = "impurity"
)

# mtry = 10
tune_grid_mtry10 <- expand.grid(
  .mtry = 10,
  .splitrule = "variance",
  .min.node.size = 1
)

set.seed(1234)
rf_model_mtry10 <- train(
  log_salary ~ .,
  method = "ranger",
  data = hitters_data,
  tuneGrid = tune_grid_mtry10,
  importance = "impurity"
)
```

* Variable importance of `mtry = 2`

```{r fig.align="center"}
plot(varImp(rf_model_mtry2))
```

The most important variables seem to be `CAtBat` (# of times at bat), `CRuns` (# of runs), `CHits` (# of hits), `CWalks` (# of walks) and `CRBI` (# of runs batted), all of which are related to overall career statistics. These variables are relatively close to each other in terms of importance.

* Variable importance of `mtry = 10`

```{r fig.align="center"}
plot(varImp(rf_model_mtry10))
```

`CAtBat` and `CHits` are still on the top of the list alongside with `CRuns`, but the steps are distributed more spaciously. The top 5 is the same as it was for `mtry = 2`, but with differing importance.

#### b. One of them is more extreme in terms of how the most important and the next ones relate to each other. Give an intuitive explanation how mtry relates to relative importance of variables in random forest models.

Splitting happens on the best possible variables at each step, and more important variables are selected more frequently, while less important variables will be left out more often. Since with `mtry = 2` we're sampling two variables out of all possible ones randomly, at a given decision point any one variable can be more important than another, giving otherwise less important variables bigger weights, and not letting especially important variables stick out. With `mtry = 10` however, out of the 19 variables that we have, we will always select 10 and assign weights to them, which means there's a bigger chance of selecting variables that are actually important from the model's perspective. This is why it's more apparent in the second case that `CAtBat` is the most important variable, it is being picked more times and thus given more emphasis than with a smaller `mtry`.

#### c. In the same vein, estimate two gbm models and set `bag.fraction` to 0.1 first and to 0.9 in the second. The `tuneGrid` should consist of the same values for the two models (a dataframe with one row): `n.trees = 500, interaction.depth = 5, shrinkage = 0.1, n.minobsinnode = 5`. Compare variable importance plots for the two models. What is the meaning of `bag.fraction`? Based on this, why is one variable importance profile more extreme than the other?

```{r}
gbm_tune_grid <- expand.grid(
  n.trees = 500,
  interaction.depth = 5,
  shrinkage = 0.1,
  n.minobsinnode = 5
)

# bag.fraction = 0.1
set.seed(1234)
gbm_model_bf1 <- train(
  log_salary ~ .,
  method = "gbm",
  data = hitters_data,
  trControl = train_control,
  tuneGrid = gbm_tune_grid,
  bag.fraction = 0.1,
  verbose = FALSE
)

# bag.fraction = 0.9
set.seed(1234)
gbm_model_bf9 <- train(
  log_salary ~ .,
  method = "gbm",
  data = hitters_data,
  trControl = train_control,
  tuneGrid = gbm_tune_grid,
  bag.fraction = 0.9,
  verbose = FALSE
)
```

* Variable importance of `bag.fraction = 0.1`

```{r fig.align="center"}
plot(varImp(gbm_model_bf1))
```

With `bag.fraction = 0.1` the variable importance plot is significantly different than it was in case of the random forests. Variables such as `PutOuts`, `Assists` and `Years` are ranked on top, whereas variables deemed important by RF are in lower positions.

* Variable importance of `bag.fraction = 0.9`

```{r fig.align="center"}
plot(varImp(gbm_model_bf9))
```

It all seem to return to "normal" with `bag.fraction = 0.9`, but `CAtBat` stuck out, with `CHits` and `CRuns` not even hitting the 40% importance mark. This is a more extreme plot, putting almost all emphasis on `CAtBat`.

`bag.fraction` determines what portion of the sample is used to construct each of the trees. In case of 0.1, each tree is built based on 10% of the data, which leads to a behavior similar to that of `mtry = 2` from before. In a smaller portion of the data, any variable can gain significance and therefore weight, which will then in turn influence its overall importance. While when we're sampling 90% of the data, we're considering almost every aspect for each tree, only leaving out a small amount of observations. `CAtBat` seems to be very important for each of the trees, and therefore overshadows all other variables, just like in case of `mtry = 10`.

# 3. Stacking

```{r, include=FALSE}
no_show_data <- fread("data/no-show-data.csv")

# some data cleaning
no_show_data[, c("PatientId", "AppointmentID", "Neighbourhood") := NULL]
setnames(no_show_data, 
         c("No-show", 
           "Age", 
           "Gender",
           "ScheduledDay", 
           "AppointmentDay",
           "Scholarship",
           "Hipertension",
           "Diabetes",
           "Alcoholism",
           "Handcap",
           "SMS_received"), 
         c("no_show", 
           "age", 
           "gender", 
           "scheduled_day", 
           "appointment_day",
           "scholarship",
           "hypertension",
           "diabetes",
           "alcoholism",
           "handicap",
           "sms_received"))

# for binary prediction, the target variable must be a factor
no_show_data[, no_show := factor(no_show, levels = c("Yes", "No"))]
no_show_data[, handicap := ifelse(handicap > 0, 1, 0)]

# create new variables
no_show_data[, gender := factor(gender)]
no_show_data[, scholarship := factor(scholarship)]
no_show_data[, hypertension := factor(hypertension)]
no_show_data[, alcoholism := factor(alcoholism)]
no_show_data[, handicap := factor(handicap)]

no_show_data[, scheduled_day := as.Date(scheduled_day)]
no_show_data[, appointment_day := as.Date(appointment_day)]
no_show_data[, days_since_scheduled := as.integer(appointment_day - scheduled_day)]

# clean up a little bit
no_show_data <- no_show_data[age %between% c(0, 95)]
no_show_data <- no_show_data[days_since_scheduled > -1]
no_show_data[, c("scheduled_day", "appointment_day", "sms_received") := NULL]
```

```{r}
library(h2o)
h2o.init(max_mem_size = "8g")
no_show_data <- as.h2o(no_show_data)
```

#### a. Create train / validation / test sets, cutting the data into 5% - 45% - 50% parts.

```{r}
splitted_data <- h2o.splitFrame(
  no_show_data,
  ratios = c(0.05, 0.5),
  seed = 1234)

no_show_data_train <- splitted_data[[1]]
no_show_data_valid <- splitted_data[[2]]
no_show_data_test <- splitted_data[[3]]
```

#### b. Train a benchmark model of your choice using `h2o` (such as random forest, gbm or glm) and evaluate it on the validation set.

```{r}
y <- "no_show"
X <- setdiff(names(no_show_data_train), y)

rf_params <- list(
  ntrees = 1000,
  mtries = c(2, 3, 5)
)

rf_grid <- h2o.grid(
  x = X, 
  y = y, 
  training_frame = no_show_data_train, 
  algorithm = "randomForest",
  nfolds = 5,
  seed = 1234,
  hyper_params = rf_params
)

# Best random forest model
rf_model <- h2o.getModel(h2o.getGrid(rf_grid@grid_id)@model_ids[[1]])

# Evaluate on the validation set
h2o.rmse(h2o.performance(rf_model, newdata = no_show_data_valid))
```

#### c. Build at least 4 models of __different families__ using cross validation, keeping cross validated predictions. One of the model families must be `deeplearning` (you can try, for example, different network topologies).

#### d. Evaluate validation set performance of each model.

#### e. How large are the correlations of predicted scores of the validation set produced by the base learners?

#### f. Create a stacked ensemble model from the base learners. Experiment with at least two different ensembling meta learners.

#### g. Evaluate ensembles on validation set. Did it improve prediction?

#### h. Evaluate the best performing model on the test set. How does performance compare to that of the validation set?
