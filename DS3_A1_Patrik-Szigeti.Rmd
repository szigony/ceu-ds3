---
title: "Data Science 3: Homework Assignment 1"
author: "Patrik Szigeti"
date: '03/08/2020'
output: html_document
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r, include = FALSE, message = FALSE, warning = FALSE}
# Load libraries
library(data.table)
library(magrittr)
library(caret)
library(rpart)
library(rpart.plot)
library(xgboost)
library(ranger)
library(gbm)
library(ISLR)
library(skimr)
library(ROCR)
library(GGally)
```

# 1. Tree ensemble models

```{r}
# Read in the OJ dataset
oj_data <- data.table(OJ)
```

```{r, include=FALSE}
skim(oj_data)
```

Our goal is to predict `Purchase`, which is a factor with two possible outcomes: __CH__ (Citrus Hill) and __MM__ (Minute Maid) orange juices. We want to know which orange juice is picked in a certain situation. We have 1070 observations and 18 variables in the dataset.

#### a. Create a training data of 75% and keep 25% of the data as a test set. Train a decision tree as a benchmark model. Plot the final model and interpret the result.

* Create training data (75%) and test data (25%)

```{r}
set.seed(1234)
train_indices <- createDataPartition(
  y = oj_data[["Purchase"]],
  times = 1,
  p = 0.75,
  list = FALSE
)

oj_data_train <- oj_data[train_indices, ]
oj_data_test <- oj_data[-train_indices, ]
```

* Train a decision tree as the benchmark model

```{r}
train_control <- trainControl(
  method = "repeatedcv",
  number = 5,
  repeats = 3
)

set.seed(1234)
simple_tree_model <- train(
  Purchase ~ .,
  method = "rpart",
  data = oj_data_train,
  tuneGrid = data.frame(cp = c(0.01, 0.02, 0.05)),
  trControl = train_control)
simple_tree_model
```

* Plot the final model

```{r fig.align="center"}
rpart.plot(simple_tree_model[["finalModel"]])
```

The simple decision tree considers four variables as splitting points: `LoyalCH` (customer brand loyalty for CH), `PriceDiff` (sale price of MM minus sale price of CH), `SpecialCH` (indicator of special on CH) and `WeekofPurchase`.

* Not surprisingly, if someone is more loyal to CH, they tend to choose CH (in 55% of the cases).
* If the customer's loyalty towards CH drops under 0.28, they will end up buying MM (22% of the whole population).
* The emphasis then falls to `PriceDiff`. 52% will still buy CH, even if it costs more than MM by at maximum 0.39. After that, loyalty plays a smaller factor, but CH fanatics will still buy CH, dispite the price difference.
* It's interesting to see that even if MM costs less, the week of purchase can pay a factor in people ending buying MM instead.
* If CH is cheaper, and there is a special on CH (`SpecialCH = 1`), out of those people who are less than 50%, but more than 28% loyal to CH, 8% will end up buying MM, which isn't usual customer behavior, yet still is the case.

#### b. Investigate tree ensemble models: random forest, gradient boosting machine, XGBoost. Try various tuning parameter combinations and select the best model using cross-validation.

* Random forest

Since "variance" splitrule is only applicable to regression data, and this is a classification problem, I tried three possible splitting rules from the `ranger` package: `gini`, `extratrees` and `hellinger` to determine which one performs best in this case.

```{r}
tune_grid_gini <- expand.grid(
  .mtry = c(2, 3, 5, 7, 9, 11, 13, 15),
  .splitrule = "gini",
  .min.node.size = c(5, 10)
)

tune_grid_et <- expand.grid(
  .mtry = c(2, 3, 5, 7, 9, 11, 13, 15),
  .splitrule = "extratrees",
  .min.node.size = c(5, 10)
)

tune_grid_h <- expand.grid(
  .mtry = c(2, 3, 5, 7, 9, 11, 13, 15),
  .splitrule = "hellinger",
  .min.node.size = c(5,10)
)

# Random forest with "gini" splitrule
set.seed(1234)
rf_model_gini <- train(
  Purchase ~ .,
  method = "ranger",
  data = oj_data_train,
  trControl = train_control,
  tuneGrid = tune_grid_gini,
  importance = "impurity"
)

# Random forest with "extratrees" splitrule
set.seed(1234)
rf_model_et <- train(
  Purchase ~ .,
  method = "ranger",
  data = oj_data_train,
  trControl = train_control,
  tuneGrid = tune_grid_et,
  importance = "impurity")

# Random forest with "hellinger" splitrule
set.seed(1234)
rf_model_h <- train(
  Purchase ~ .,
  method = "ranger",
  data = oj_data_train,
  trControl = train_control,
  tuneGrid = tune_grid_h,
  importance = "impurity")
```

Now let's see how they compare to each other.

```{r}
resamples(list("gini" = rf_model_gini, 
               "extratrees" = rf_model_et, 
               "hellinger" = rf_model_h)) %>% 
  summary()
```

There are only small differences, but based on the mean accuracy, `extratrees` only comes close to the default Gini-index, and is about the same as `hellinger`, so I decided to use the default `gini`.

```{r}
rf_model <- rf_model_gini
```

* Gradient boosting machine

```{r}
gbm_grid <- expand.grid(
  n.trees = c(750, 1000, 1250), 
  interaction.depth = c(1, 2, 3), 
  shrinkage = c(0.005, 0.01, 0.015, 0.02),
  n.minobsinnode = 1)

set.seed(1234)
gbm_model <- train(
  Purchase ~ .,
  method = "gbm",
  data = oj_data_train,
  trControl = train_control,
  tuneGrid = gbm_grid,
  verbose = FALSE)

gbm_model
```

* XGBoost

```{r}
xgb_grid <- expand.grid(
  nrounds = c(500, 1000),
  max_depth = c(2, 3, 5),
  eta = c(0.01, 0.05),
  gamma = 0,
  colsample_bytree = c(0.5, 0.7),
  min_child_weight = 1,
  subsample = 0.5)

set.seed(1234)
xgboost_model <- train(
  Purchase ~ .,
  method = "xgbTree",
  data = oj_data_train,
  trControl = train_control,
  tuneGrid = xgb_grid)

xgboost_model
```

#### c. Compare different models with the `resamples` function (make sure to set the same seed before model training for all 3 models). Is any of these giving significantly different predictive power than the others?

```{r}
resamples_object <- resamples(list("simple_tree" = simple_tree_model,
                                   "random_forest" = rf_model,
                                   "gbm" = gbm_model,
                                   "xgboost" = xgboost_model))
summary(resamples_object)
```

Based on the mean accuracy, random forest has the worst predictive power out of the four models, but even that isn't significantly worse. It's interesting to see, however, that even the simple tree model performs better on the train dataset. GBM and XGBoost perform the best (both with 0.83 accuracy), but the trade-off for XGBoost's computational needs doesn't seem to be worth it, as GBM is slightly better. I'm selecting _GBM_ as the best predictor.

#### d. Choose the best model and plot ROC curve for the best model on the test set. Calculate and interpret AUC.

```{r fig.align="center"}
predicted_probabilities <- predict(gbm_model, newdata = oj_data_test, type = "prob")
rocr_prediction <- prediction(predicted_probabilities[["MM"]], oj_data_test[["Purchase"]])
plot(performance(rocr_prediction, "tpr", "fpr"), col = "black")
abline(a = 0, b = 1, col = "#8AB63F")
```

```{r}
performance(rocr_prediction, measure = "auc")@y.values[[1]]
```

The green line represents a uniformative test where AUC = 0.5. The AUC of the GBM model is 0.9, which indicates that we can predict the type of orange juice a customer is going to buy pretty well, the model is capable of distinguishing between the two classes with more than a 90% chance.

#### e. Inspect variable importance plots for the 3 models. Are similar variables found to be the most important for the 3 models?

* Random forest

```{r fig.align="center"}
plot(varImp(rf_model))
```

The most important variable for the random forest is by far `LoyalCH`, which measures customer brand loyalty for CH. It's followed by `WeekofPurchase` and `StoreID`. Anything price-related is less important than these variables. `LoyalCH` has extreme importance in case of the random forest compared to any of the other variables.

* GBM

```{r fig.align="center"}
plot(varImp(gbm_model))
```

For the GBM model, `LoyalCH` is still the most important factor, however, `PriceDiff` preceeds `WeekofPurchase` and `Store ID`. With GBM, the order of `ListPriceDiff` and `SalePriceMM` are switched compared to the random forest. `LoyalCH`'s importance is still extreme.

* XGBoost

```{r fig.align="center"}
plot(varImp(xgboost_model))
```

The 6 most important variables are identical between GBM and XGBoost, and the ratios seem to be almost identical as well.

Altogether, each of the three models operates with very similar variable importances with minor differences, with `LoyalCH` being by far the most important variable.

# 2. Variable importance profiles

```{r}
hitters_data <- data.table(Hitters)
hitters_data <- hitters_data[!is.na(Salary)]
hitters_data[, log_salary := log(Salary)]
hitters_data[, Salary := NULL]
```

#### a. Train two random forest models: one with mtry = 2 and another with mtry = 10 (use the whole dataset and don't use cross-validation). Inspect variable importance profiles. What do you see in terms of how important the first few variables are relative to each other?

```{r}
# mtry = 2
tune_grid_mtry2 <- expand.grid(
  .mtry = 2,
  .splitrule = "variance",
  .min.node.size = 1
)

set.seed(1234)
rf_model_mtry2 <- train(
  log_salary ~ .,
  method = "ranger",
  data = hitters_data,
  tuneGrid = tune_grid_mtry2,
  importance = "impurity"
)

# mtry = 10
tune_grid_mtry10 <- expand.grid(
  .mtry = 10,
  .splitrule = "variance",
  .min.node.size = 1
)

set.seed(1234)
rf_model_mtry10 <- train(
  log_salary ~ .,
  method = "ranger",
  data = hitters_data,
  tuneGrid = tune_grid_mtry10,
  importance = "impurity"
)
```

* Variable importance of `mtry = 2`

```{r fig.align="center"}
plot(varImp(rf_model_mtry2))
```

The most important variables seem to be `CAtBat` (# of times at bat), `CRuns` (# of runs), `CHits` (# of hits), `CWalks` (# of walks) and `CRBI` (# of runs batted), all of which are related to overall career statistics. These variables are relatively close to each other in terms of importance.

* Variable importance of `mtry = 10`

```{r fig.align="center"}
plot(varImp(rf_model_mtry10))
```

`CAtBat` and `CHits` are still on the top of the list alongside with `CRuns`, but the steps are distributed more spaciously. The top 5 is the same as it was for `mtry = 2`, but with differing importance.

#### b. One of them is more extreme in terms of how the most important and the next ones relate to each other. Give an intuitive explanation how mtry relates to relative importance of variables in random forest models.

Splitting happens on the best possible variables at each step, and more important variables are selected more frequently, while less important variables will be left out more often. Since with `mtry = 2` we're sampling two variables out of all possible ones randomly, at a given decision point any one variable can be more important than another, giving otherwise less important variables bigger weights, and not letting especially important variables stick out. With `mtry = 10` however, out of the 19 variables that we have, we will always select 10 and assign weights to them, which means there's a bigger chance of selecting variables that are actually important from the model's perspective. This is why it's more apparent in the second case that `CAtBat` is the most important variable, it is being picked more times and thus given more emphasis than with a smaller `mtry`.

#### c. In the same vein, estimate two gbm models and set `bag.fraction` to 0.1 first and to 0.9 in the second. The `tuneGrid` should consist of the same values for the two models (a dataframe with one row): `n.trees = 500, interaction.depth = 5, shrinkage = 0.1, n.minobsinnode = 5`. Compare variable importance plots for the two models. What is the meaning of `bag.fraction`? Based on this, why is one variable importance profile more extreme than the other?

```{r}
gbm_tune_grid <- expand.grid(
  n.trees = 500,
  interaction.depth = 5,
  shrinkage = 0.1,
  n.minobsinnode = 5
)

# bag.fraction = 0.1
set.seed(1234)
gbm_model_bf1 <- train(
  log_salary ~ .,
  method = "gbm",
  data = hitters_data,
  trControl = train_control,
  tuneGrid = gbm_tune_grid,
  bag.fraction = 0.1,
  verbose = FALSE
)

# bag.fraction = 0.9
set.seed(1234)
gbm_model_bf9 <- train(
  log_salary ~ .,
  method = "gbm",
  data = hitters_data,
  trControl = train_control,
  tuneGrid = gbm_tune_grid,
  bag.fraction = 0.9,
  verbose = FALSE
)
```

* Variable importance of `bag.fraction = 0.1`

```{r fig.align="center"}
plot(varImp(gbm_model_bf1))
```

With `bag.fraction = 0.1` the variable importance plot is significantly different than it was in case of the random forests. Variables such as `PutOuts`, `Assists` and `Years` are ranked on top, whereas variables deemed important by RF are in lower positions.

* Variable importance of `bag.fraction = 0.9`

```{r fig.align="center"}
plot(varImp(gbm_model_bf9))
```

It all seem to return to "normal" with `bag.fraction = 0.9`, but `CAtBat` sticks out, with `CHits` and `CRuns` barely hitting the 40% importance mark. This is a more extreme plot, putting almost all emphasis on `CAtBat`.

`bag.fraction` determines what portion of the sample is used to construct each of the trees. In case of 0.1, each tree is built based on 10% of the data, which leads to a behavior similar to that of `mtry = 2` from before. In a smaller portion of the data, any variable can gain significance and therefore weight, which will then in turn influence its overall importance. While when we're sampling 90% of the data, we're considering almost every aspect for each tree, only leaving out a small amount of observations. `CAtBat` seems to be very important for each of the trees, and therefore overshadows all other variables, just like in case of `mtry = 10`.

# 3. Stacking

```{r, include=FALSE}
data <- fread("data/no-show-data.csv")

# Some data cleaning
data[, c("PatientId", "AppointmentID", "Neighbourhood") := NULL]
setnames(data, 
         c("No-show", 
           "Age", 
           "Gender",
           "ScheduledDay", 
           "AppointmentDay",
           "Scholarship",
           "Hipertension",
           "Diabetes",
           "Alcoholism",
           "Handcap",
           "SMS_received"), 
         c("no_show", 
           "age", 
           "gender", 
           "scheduled_day", 
           "appointment_day",
           "scholarship",
           "hypertension",
           "diabetes",
           "alcoholism",
           "handicap",
           "sms_received"))

# For binary prediction, the target variable must be a factor
data[, no_show := factor(no_show, levels = c("Yes", "No"))]
data[, handicap := ifelse(handicap > 0, 1, 0)]

# Create new variables
data[, gender := factor(gender)]
data[, scholarship := factor(scholarship)]
data[, hypertension := factor(hypertension)]
data[, alcoholism := factor(alcoholism)]
data[, handicap := factor(handicap)]

data[, scheduled_day := as.Date(scheduled_day)]
data[, appointment_day := as.Date(appointment_day)]
data[, days_since_scheduled := as.integer(appointment_day - scheduled_day)]

# Clean up a little bit
data <- data[age %between% c(0, 95)]
data <- data[days_since_scheduled > -1]
data[, c("scheduled_day", "appointment_day", "sms_received") := NULL]

# Save cleaned dataset
write.csv(data, "data/no_show_data.csv")
```

```{r, warning=FALSE}
library(h2o)
h2o.no_progress() # suppress progress bars in the outcome
h2o.init(max_mem_size = "6g")
no_show_data <- h2o.importFile("data/no_show_data.csv")
```

#### a. Create train / validation / test sets, cutting the data into 5% - 45% - 50% parts.

```{r}
splitted_data <- h2o.splitFrame(
  no_show_data,
  ratios = c(0.05, 0.5),
  seed = 1234)

no_show_data_train <- splitted_data[[1]]
no_show_data_valid <- splitted_data[[2]]
no_show_data_test <- splitted_data[[3]]
```

#### b. Train a benchmark model of your choice using `h2o` (such as random forest, gbm or glm) and evaluate it on the validation set.

I trained a random forest model as my benchmark.

```{r}
y <- "no_show"
X <- setdiff(names(no_show_data_train), y)

rf_grid <- h2o.grid(
  x = X, y = y, 
  training_frame = no_show_data_train, 
  algorithm = "randomForest",
  nfolds = 5,
  seed = 1234,
  hyper_params = list(
    ntrees = 1000,
    mtries = c(2, 3, 5)
  ),
  keep_cross_validation_predictions = TRUE
)

# Best random forest model
rf_model <- h2o.getModel(h2o.getGrid(rf_grid@grid_id)@model_ids[[1]])

# Evaluate on the validation set
print(h2o.auc(h2o.performance(rf_model, newdata = no_show_data_valid)))
```

#### c. Build at least 4 models of __different families__ using cross validation, keeping cross validated predictions. One of the model families must be `deeplearning` (you can try, for example, different network topologies).

A random forest model has already been trained in the previous step, in this step, I'm training three additional base learners: GLM, GBM and deep learning.

```{r}
# GLM model
glm_model <- h2o.glm(
  X, y,
  training_frame = no_show_data_train,
  family = "binomial",
  alpha = 1,
  lambda_search = TRUE,
  seed = 1234,
  nfolds = 5, 
  keep_cross_validation_predictions = TRUE
)

# GBM model
gbm_model <- h2o.gbm(
  X, y,
  training_frame = no_show_data_train,
  ntrees = 500, 
  max_depth = 10, 
  learn_rate = 0.1, 
  seed = 1234,
  nfolds = 5, 
  keep_cross_validation_predictions = TRUE
)

# Deep learning model
dl_model <- h2o.deeplearning(
  X, y,
  training_frame = no_show_data_train,
  hidden = c(32, 8),
  seed = 1234,
  nfolds = 5, 
  keep_cross_validation_predictions = TRUE
)
```

#### d. Evaluate validation set performance of each model.

```{r}
validation_performances <- list(
  "rf" = h2o.auc(h2o.performance(rf_model, newdata = no_show_data_valid)),
  "glm" = h2o.auc(h2o.performance(glm_model, newdata = no_show_data_valid)),
  "gbm" = h2o.auc(h2o.performance(gbm_model, newdata = no_show_data_valid)),
  "dl" = h2o.auc(h2o.performance(dl_model, newdata = no_show_data_valid))
)

validation_performances
```

The validation set performance seems to be best for the random forest model, followed closely by the GBM model. The deep learning model is relatively close to these two in terms of AUC, but GLM is significantly worse.

#### e. How large are the correlations of predicted scores of the validation set produced by the base learners?

```{r fig.align="center"}
ensemble_model <- h2o.stackedEnsemble(
  X, y,
  training_frame = no_show_data_train,
  base_models = list(
    rf_model,
    glm_model, 
    gbm_model,
    dl_model
  ),
  keep_levelone_frame = TRUE,
  seed = 1234)

level_1_features <- h2o.getFrame(ensemble_model@model$levelone_frame_id$name)

level_1_dt <- as.data.table(level_1_features)[, 1:4]
setnames(level_1_dt, c("rf", "glm", "gbm", "dl"))

ggcorr(level_1_dt, label = TRUE, label_round = 2)
```

The correlation is highest between the random forest model and GBM based on the predictions on the validation set. GLM and GBM seem to mostly contradict each other, while the deep learning model's predictions correlate with GLM and RF about the same amount.

#### f. Create a stacked ensemble model from the base learners. Experiment with at least two different ensembling meta learners.

* Stacked ensemble model with `gbm` meta-learner

```{r}
ensemble_model_gbm <- h2o.stackedEnsemble(
  X, y,
  training_frame = no_show_data_train,
  metalearner_algorithm = "gbm",
  seed = 1234,
  base_models = list(
    rf_model,
    glm_model, 
    gbm_model,
    dl_model
  )
)
```

* Stacked ensemble model with `deeplearning` meta-learner

```{r}
ensemble_model_dl <- h2o.stackedEnsemble(
  X, y,
  training_frame = no_show_data_train,
  metalearner_algorithm = "deeplearning",
  seed = 1234,
  base_models = list(
    rf_model,
    glm_model, 
    gbm_model,
    dl_model
  )
)
```

#### g. Evaluate ensembles on validation set. Did it improve prediction?

```{r}
print(h2o.auc(h2o.performance(ensemble_model_gbm, newdata = no_show_data_valid)))
print(h2o.auc(h2o.performance(ensemble_model_dl, newdata = no_show_data_valid)))
```

The ensemble model with `gbm` meta-learner is slightly better compared to the simple random forest model, with the `deeplearning` meta-learner overshining both of them. It shows more than 2% improvement compared to the base deep learning model, and performs better on the validation dataset than the random forest did.

#### h. Evaluate the best performing model on the test set. How does performance compare to that of the validation set?

```{r}
print(h2o.auc(h2o.performance(ensemble_model_dl, newdata = no_show_data_test)))
```

The best performing model is the ensemble model with `deeplearning` meta-learner, which has about the same AUC on the test set, as it did on the validation set. The AUC of the model indicates that we have a better than 70% chance of predicting whether a patient will actually show up to their appointment.
